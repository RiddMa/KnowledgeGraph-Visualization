import json
import logging
from datetime import datetime

import scrapy
from bs4 import BeautifulSoup
from scrapy.crawler import CrawlerProcess

from CVE_Bot.utils.db import mg
from bot_root_dir import get_log_dir
from custom_logger import setup_logger


def strip_text(text: str):
    while text != text.strip('\n').strip(' '):
        text = text.strip('\n').strip(' ')
    return text


class EdbSpider(scrapy.Spider):
    name = 'EdbBot'
    allowed_domains = ['exploit-db.com']
    # start_urls = ['https://www.google.com']

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        log_filename = 'exploit-db-' + datetime.now().strftime("%Y-%m-%d-%H-%M-%S") + '.log'
        self.mylogger = setup_logger('edb', log_filename, level_stdout=logging.INFO)
        self.mylogger.info('Logging to file ' + str(get_log_dir().joinpath(log_filename)))

    def start_requests(self):
        self.mylogger.info('Start first request')
        yield scrapy.Request('https://www.exploit-db.com/exploits/9994',
                             headers={'Referer': 'https://www.exploit-db.com',
                                      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'},
                             meta={'playwright': True})

    def parse(self, response, **kwargs):
        soup = BeautifulSoup(response.body, 'html.parser')
        self.mylogger.info('Parsing ' + response.url)
        edb_id = response.url.strip('https://www.exploit-db.com/exploits/')
        try:
            mg.save_edb_html(edb_id, json.dumps(response.text))
            # if response.url.startswith('https://www.exploit-db.com/exploits'):
            main_panel = soup.select_one(
                'body>div>div.main-panel>div.content>div>div>div:nth-child(1)>div>div.ml-2.mr-2')
            info_panel = main_panel.select_one('div:nth-child(1)')
            next_href = main_panel.select_one('div:nth-child(2)>div.col>div>a')['href']
            card1 = info_panel.select_one('div:nth-child(1)>div>div.card-body>div>div>div')
            card2 = info_panel.select_one('div:nth-child(2)>div>div.card-body>div>div>div')
            card3 = info_panel.select_one('div:nth-child(3)>div>div.card-body>div>div>div')
            exploit = {
                'edb_id': strip_text(card1.select_one('div:nth-child(1)>h6').text),
                'title': strip_text(soup.select_one(
                    'body>div>div.main-panel>div.content>div>div>div:nth-child(1)>div>div.row.justify-content-md-center>h1').text),
                'cve_ids': [strip_text(a.text) for a in card1.select('div:nth-child(2)>h6>a')],
                'author': strip_text(card2.select_one('div:nth-child(1)>h6').text),
                'type': strip_text(card2.select_one('div:nth-child(2)>h6').text),
                'platform': strip_text(card3.select_one('div:nth-child(1)>h6').text),
                'date': strip_text(card3.select_one('div:nth-child(2)>h6').text),
                'code': json.dumps(strip_text(soup.select_one(
                    'body>div>div.main-panel>div.content>div>div>div:nth-child(2)>div.card-body').text))
            }
            # [strip_text(a.text) for a in card1.select('div:nth-child(2)>h6>a')]
            mg.save_edb_json(edb_id, exploit)
            if next_href != "/exploits/#":
                self.mylogger.info('Start request ' + 'https://www.exploit-db.com' + next_href)
                yield scrapy.Request('https://www.exploit-db.com' + next_href,
                                     headers={'Referer': 'https://www.exploit-db.com',
                                              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'},
                                     )
        except BaseException as err:
            self.mylogger.error(edb_id + ': ' + str(err))


if __name__ == "__main__":
    process = CrawlerProcess(
        settings={
            "TWISTED_REACTOR":
                "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
            "DOWNLOAD_HANDLERS": {
                "https":
                    "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
                "http":
                    "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
            },
            "DOWNLOAD_DELAY": 0,
            "CONCURRENT_REQUESTS_PER_DOMAIN": 6,
        })
    process.crawl(EdbSpider)
    process.start()
