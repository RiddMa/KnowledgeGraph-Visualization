import scrapy
from bs4 import BeautifulSoup
from scrapy.crawler import CrawlerProcess


class EdbSpider(scrapy.Spider):
    name = 'EdbBot'
    allowed_domains = ['exploit-db.com']
    start_urls = ['https://www.exploit-db.com/search']

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def parse(self, response, **kwargs):
        soup = BeautifulSoup(response.body, 'html.parser')
        table = soup.select('#exploits-table > tbody:nth-child(2) > tr')
        pass


if __name__ == "__main__":
    process = CrawlerProcess(
        settings={
            "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
            "DOWNLOAD_HANDLERS": {
                "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
                "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
            },
            'PLAYWRIGHT_BROWSER_TYPE': 'firefox',
            'PLAYWRIGHT_LAUNCH_OPTIONS': {
                'headless': False
            },
            "DOWNLOAD_DELAY": 1,
            "CONCURRENT_REQUESTS_PER_DOMAIN": 1,
        }
    )
    process.crawl(EdbSpider)
    process.start()
